#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
AIæ–°é—»æ—¥æŠ¥æœºå™¨äººä¸»ç¨‹åº
æ¯æ—¥è‡ªåŠ¨èšåˆAIç›¸å…³æ–°é—»å¹¶æ¨é€åˆ°é’‰é’‰
"""

import json
import logging
import requests
import feedparser
import hmac
import hashlib
import time
import base64
from datetime import datetime, timedelta
from typing import List, Dict, Optional
from concurrent.futures import ThreadPoolExecutor, as_completed

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('ai_news_bot.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


@dataclass
class NewsItem:
    """æ–°é—»æ¡ç›®æ•°æ®ç»“æ„"""
    title: str
    summary: str
    url: str
    source: str
    published: datetime
    category: str
    importance_score: float = 0.0


class NewsAggregator:
    """æ–°é—»èšåˆå™¨"""
    
    def __init__(self, config_path: str = "config.json"):
        with open(config_path, 'r', encoding='utf-8') as f:
            self.config = json.load(f)
        
        self.webhook_url = self.config["dingtalk_webhook"]
        self.secret = self.config.get("dingtalk_secret", "")  # åŠ ç­¾å¯†é’¥ï¼ˆå¯é€‰ï¼‰
        self.sent_urls_file = "sent_urls.json"
        self.sent_urls = self._load_sent_urls()
        
    def _load_sent_urls(self) -> set:
        """åŠ è½½å·²å‘é€çš„æ–°é—»URL"""
        try:
            with open(self.sent_urls_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                return set(data.get('sent_urls', []))
        except FileNotFoundError:
            return set()
    
    def _save_sent_urls(self):
        """ä¿å­˜å·²å‘é€çš„æ–°é—»URL"""
        data = {
            'sent_urls': list(self.sent_urls),
            'last_update': datetime.now().isoformat()
        }
        with open(self.sent_urls_file, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
    
    def _get_url_hash(self, url: str) -> str:
        """ç”ŸæˆURLçš„å“ˆå¸Œå€¼ç”¨äºå»é‡"""
        return hashlib.md5(url.encode()).hexdigest()
    
    def _is_duplicate(self, url: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦é‡å¤æ–°é—»"""
        url_hash = self._get_url_hash(url)
        return url_hash in self.sent_urls
    
    def _calculate_importance_score(self, title: str, keywords: List[str]) -> float:
        """è®¡ç®—æ–°é—»é‡è¦æ€§è¯„åˆ†"""
        score = 0.0
        title_lower = title.lower()
        
        # åŸºç¡€åˆ†æ•°
        score += 1.0
        
        # å…³é”®è¯åŒ¹é…åŠ åˆ†
        for keyword in keywords:
            if keyword.lower() in title_lower:
                score += 2.0
        
        # ç‰¹æ®Šå…³é”®è¯é¢å¤–åŠ åˆ†
        high_value_keywords = [
            'openai', 'gpt-4', 'claude', 'anthropic', 'funding', 'acquisition',
            'breakthrough', 'research', 'paper'
        ]
        for keyword in high_value_keywords:
            if keyword in title_lower:
                score += 3.0
        
        return score
    
    def _fetch_rss_feed(self, source: Dict) -> List[NewsItem]:
        """æŠ“å–RSSæº"""
        try:
            logger.info(f"æ­£åœ¨æŠ“å–: {source['name']}")
            
            # æ·»åŠ è¯·æ±‚å¤´æ¨¡æ‹Ÿæµè§ˆå™¨
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            
            response = requests.get(source['url'], headers=headers, timeout=15)
            response.raise_for_status()
            
            feed = feedparser.parse(response.content)
            news_items = []
            
            # è·å–24å°æ—¶å†…çš„æ–°é—»
            cutoff_time = datetime.now() - timedelta(hours=24)
            
            for entry in feed.entries[:50]:  # é™åˆ¶æ•°é‡é¿å…è¿‡å¤š
                try:
                    # è§£æå‘å¸ƒæ—¶é—´
                    published = datetime.now()
                    if hasattr(entry, 'published_parsed') and entry.published_parsed:
                        published = datetime(*entry.published_parsed[:6])
                    elif hasattr(entry, 'updated_parsed') and entry.updated_parsed:
                        published = datetime(*entry.updated_parsed[:6])
                    
                    # æ£€æŸ¥æ˜¯å¦åœ¨æ—¶é—´èŒƒå›´å†…
                    if published < cutoff_time:
                        continue
                    
                    title = getattr(entry, 'title', '').strip()
                    url = getattr(entry, 'link', '').strip()
                    
                    if not title or not url:
                        continue
                    
                    # å»é‡æ£€æŸ¥
                    if self._is_duplicate(url):
                        continue
                    
                    # è®¡ç®—é‡è¦æ€§è¯„åˆ†
                    importance_score = self._calculate_importance_score(
                        title, source.get('keywords', [])
                    )
                    
                    # ç”Ÿæˆæ‘˜è¦
                    summary = getattr(entry, 'summary', '')
                    if not summary and hasattr(entry, 'description'):
                        summary = getattr(entry, 'description', '')
                    
                    # æ¸…ç†HTMLæ ‡ç­¾
                    summary = self._clean_html(summary)
                    
                    news_item = NewsItem(
                        title=title,
                        summary=summary[:200] + "..." if len(summary) > 200 else summary,
                        url=url,
                        source=source['name'],
                        published=published,
                        category=source.get('category', 'general'),
                        importance_score=importance_score
                    )
                    
                    news_items.append(news_item)
                    
                except Exception as e:
                    logger.warning(f"å¤„ç†æ–°é—»æ¡ç›®æ—¶å‡ºé”™: {e}")
                    continue
            
            logger.info(f"ä» {source['name']} æŠ“å–åˆ° {len(news_items)} æ¡æ–°é—»")
            return news_items
            
        except Exception as e:
            logger.error(f"æŠ“å– RSS æºå¤±è´¥ {source['name']}: {e}")
            return []
    
    def _clean_html(self, text: str) -> str:
        """æ¸…ç†HTMLæ ‡ç­¾"""
        import re
        # ç§»é™¤HTMLæ ‡ç­¾
        text = re.sub(r'<[^>]+>', '', text)
        # ç§»é™¤å¤šä½™çš„ç©ºç™½å­—ç¬¦
        text = re.sub(r'\s+', ' ', text).strip()
        return text
    
    def fetch_all_news(self) -> List[NewsItem]:
        """æŠ“å–æ‰€æœ‰æ–°é—»æº"""
        all_news = []
        
        # å¹¶å‘æŠ“å–æ‰€æœ‰æ–°é—»æº
        with ThreadPoolExecutor(max_workers=5) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_source = {}
            
            for category, sources in self.config["news_sources"].items():
                for source in sources:
                    source['category'] = category
                    future = executor.submit(self._fetch_rss_feed, source)
                    future_to_source[future] = source
            
            # æ”¶é›†ç»“æœ
            for future in as_completed(future_to_source):
                source = future_to_source[future]
                try:
                    news_items = future.result()
                    all_news.extend(news_items)
                except Exception as e:
                    logger.error(f"è·å–æ–°é—»æ—¶å‡ºé”™ {source['name']}: {e}")
        
        # æŒ‰é‡è¦æ€§è¯„åˆ†æ’åº
        all_news.sort(key=lambda x: x.importance_score, reverse=True)
        
        logger.info(f"æ€»å…±æŠ“å–åˆ° {len(all_news)} æ¡æ–°é—»")
        return all_news


class AISummarizer:
    """AIæ–°é—»æ‘˜è¦å™¨"""
    
    def __init__(self, config: Dict):
        self.api_key = config["ai_summary"]["api_key"]
        self.model = config["ai_summary"]["model"]
        self.max_tokens = config["ai_summary"]["max_tokens"]
        self.temperature = config["ai_summary"]["temperature"]
        
    def generate_summary(self, news_item: NewsItem) -> str:
        """ä½¿ç”¨AIç”Ÿæˆæ–°é—»æ‘˜è¦"""
        if not self.api_key or self.api_key == "YOUR_OPENAI_API_KEY":
            # å¦‚æœæ²¡æœ‰APIå¯†é’¥ï¼Œè¿”å›åŸå§‹æ‘˜è¦
            return news_item.summary or "æš‚æ— æ‘˜è¦"
        
        try:
            prompt = f"""
è¯·ä¸ºä»¥ä¸‹AIç›¸å…³æ–°é—»ç”Ÿæˆä¸€ä¸ªç®€æ´çš„ä¸­æ–‡æ‘˜è¦ï¼Œè¦æ±‚ï¼š
1. é•¿åº¦æ§åˆ¶åœ¨50å­—ä»¥å†…
2. ä¿ç•™å…³é”®ä¿¡æ¯ï¼ˆå…¬å¸åã€æŠ€æœ¯åã€æ•°æ®ç­‰ï¼‰
3. è¯­è¨€å®¢è§‚ä¸“ä¸š

æ–°é—»æ ‡é¢˜ï¼š{news_item.title}
æ–°é—»å†…å®¹ï¼š{news_item.summary}

è¯·ç›´æ¥è¾“å‡ºæ‘˜è¦ï¼Œä¸éœ€è¦å…¶ä»–è¯´æ˜ã€‚
"""
            
            headers = {
                "Authorization": f"Bearer {self.api_key}",
                "Content-Type": "application/json"
            }
            
            data = {
                "model": self.model,
                "messages": [
                    {"role": "user", "content": prompt}
                ],
                "max_tokens": self.max_tokens,
                "temperature": self.temperature
            }
            
            response = requests.post(
                "https://api.openai.com/v1/chat/completions",
                headers=headers,
                json=data,
                timeout=30
            )
            
            response.raise_for_status()
            result = response.json()
            
            if "choices" in result and result["choices"]:
                return result["choices"][0]["message"]["content"].strip()
            else:
                return news_item.summary or "æš‚æ— æ‘˜è¦"
                
        except Exception as e:
            logger.error(f"AIæ‘˜è¦ç”Ÿæˆå¤±è´¥: {e}")
            return news_item.summary or "æš‚æ— æ‘˜è¦"


class DingTalkNotifier:
    """é’‰é’‰é€šçŸ¥å™¨"""
    
    def __init__(self, webhook_url: str, secret: str = ""):
        self.webhook_url = webhook_url
        self.secret = secret
    
    def _get_sign(self) -> str:
        """ç”Ÿæˆç­¾åï¼ˆå¦‚æœé…ç½®äº†secretï¼‰"""
        if not self.secret:
            return ""
        
        timestamp = str(int(time.time() * 1000))
        string_to_sign = f'{timestamp}\n{self.secret}'
        hmac_code = hmac.new(
            self.secret.encode('utf-8'),
            string_to_sign.encode('utf-8'),
            digestmod=hashlib.sha256
        ).digest()
        
        sign = base64.b64encode(hmac_code).decode('utf-8')
        return f"&timestamp={timestamp}&sign={sign}"
    
    def send_daily_news(self, news_items: List[NewsItem], date: str) -> bool:
        """å‘é€æ¯æ—¥æ–°é—»"""
        try:
            # æ„å»ºMarkdownæ¶ˆæ¯
            message = self._build_markdown_message(news_items, date)
            
            # å‘é€æ¶ˆæ¯
            data = {
                "msgtype": "markdown",
                "markdown": {
                    "title": f"AIæ¯æ—¥æ—©æŠ¥ | {date}",
                    "text": message
                }
            }
            
            headers = {
                "Content-Type": "application/json"
            }
            
            # å¦‚æœé…ç½®äº†secretï¼Œæ·»åŠ ç­¾å
            url = self.webhook_url
            if self.secret:
                url += self._get_sign()
            
            response = requests.post(url, headers=headers, json=data, timeout=30)
            response.raise_for_status()
            
            result = response.json()
            if result.get("errcode") == 0:
                logger.info("é’‰é’‰æ¶ˆæ¯å‘é€æˆåŠŸ")
                return True
            else:
                logger.error(f"é’‰é’‰æ¶ˆæ¯å‘é€å¤±è´¥: {result.get('errmsg')}")
                return False
            
        except Exception as e:
            logger.error(f"é’‰é’‰æ¶ˆæ¯å‘é€å¤±è´¥: {e}")
            return False
    
    def _build_markdown_message(self, news_items: List[NewsItem], date: str) -> str:
        """æ„å»ºMarkdownæ ¼å¼çš„æ¶ˆæ¯"""
        message_parts = []
        
        # æ ‡é¢˜
        message_parts.append(f"## ğŸ“… AI æ¯æ—¥æ—©æŠ¥ | {date}")
        message_parts.append("")
        
        # ç»Ÿè®¡ä¿¡æ¯
        message_parts.append(f"**ä»Šæ—¥ç²¾é€‰ {len(news_items)} æ¡AIè¦é—»**")
        message_parts.append("")
        
        # æ–°é—»åˆ—è¡¨
        for i, news in enumerate(news_items, 1):
            # æ ¹æ®ç±»åˆ«é€‰æ‹©emoji
            emoji = self._get_category_emoji(news.category)
            
            # æ„å»ºæ–°é—»æ¡ç›®
            news_line = f"**{emoji} {news.title}**\n"
            news_line += f"> {news.summary}\n"
            news_line += f"> ğŸ“° æ¥æºï¼š{news.source}\n"
            news_line += f"> ğŸ”— [åŸæ–‡é“¾æ¥]({news.url})\n"
            
            message_parts.append(news_line)
        
        # åº•éƒ¨ä¿¡æ¯
        message_parts.append("")
        message_parts.append("---")
        message_parts.append("*æœ¬ç®€æŠ¥ç”± AI è‡ªåŠ¨ç”Ÿæˆï¼Œä»…ä¾›å†…éƒ¨å‚è€ƒ*")
        
        return "\n".join(message_parts)
    
    def _get_category_emoji(self, category: str) -> str:
        """æ ¹æ®ç±»åˆ«è¿”å›å¯¹åº”çš„emoji"""
        emoji_map = {
            "international_media": "ğŸŒ",
            "chinese_media": "ğŸ‡¨ğŸ‡³", 
            "ai_funding": "ğŸ’°",
            "general": "ğŸ“°"
        }
        return emoji_map.get(category, "ğŸ“°")


class AINewsBot:
    """AIæ–°é—»æœºå™¨äººä¸»ç±»"""
    
    def __init__(self, config_path: str = "config.json"):
        self.config_path = config_path
        self.aggregator = NewsAggregator(config_path)
        self.summarizer = AISummarizer(self.aggregator.config)
        self.notifier = DingTalkNotifier(
            self.aggregator.webhook_url,
            self.aggregator.secret
        )
    
    def run_daily_job(self) -> bool:
        """æ‰§è¡Œæ¯æ—¥æ–°é—»æ”¶é›†å’Œæ¨é€ä»»åŠ¡"""
        try:
            logger.info("å¼€å§‹æ‰§è¡Œæ¯æ—¥æ–°é—»ä»»åŠ¡")
            
            # 1. æŠ“å–æ–°é—»
            all_news = self.aggregator.fetch_all_news()
            
            if not all_news:
                logger.warning("æœªæŠ“å–åˆ°ä»»ä½•æ–°é—»")
                return False
            
            # 2. ç­›é€‰é«˜è´¨é‡æ–°é—»
            max_news = self.aggregator.config["settings"]["max_news"]
            selected_news = all_news[:max_news]
            
            logger.info(f"ç­›é€‰å‡º {len(selected_news)} æ¡é«˜è´¨é‡æ–°é—»")
            
            # 3. ç”ŸæˆAIæ‘˜è¦
            for news in selected_news:
                news.summary = self.summarizer.generate_summary(news)
                time.sleep(1)  # é¿å…APIé¢‘ç‡é™åˆ¶
            
            # 4. å‘é€åˆ°é’‰é’‰
            current_date = datetime.now().strftime("%Yå¹´%mæœˆ%dæ—¥")
            success = self.notifier.send_daily_news(selected_news, current_date)
            
            if success:
                # 5. è®°å½•å·²å‘é€çš„URL
                for news in selected_news:
                    self.aggregator.sent_urls.add(
                        self.aggregator._get_url_hash(news.url)
                    )
                self.aggregator._save_sent_urls()
                
                logger.info("æ¯æ—¥æ–°é—»ä»»åŠ¡æ‰§è¡Œå®Œæˆ")
                return True
            else:
                logger.error("å‘é€æ¶ˆæ¯å¤±è´¥")
                return False
                
        except Exception as e:
            logger.error(f"æ‰§è¡Œæ¯æ—¥ä»»åŠ¡æ—¶å‡ºé”™: {e}")
            return False


def main():
    """ä¸»å‡½æ•°"""
    try:
        bot = AINewsBot()
        success = bot.run_daily_job()
        
        if success:
            logger.info("ç¨‹åºæ‰§è¡ŒæˆåŠŸ")
            exit(0)
        else:
            logger.error("ç¨‹åºæ‰§è¡Œå¤±è´¥")
            exit(1)
            
    except Exception as e:
        logger.error(f"ç¨‹åºå¼‚å¸¸: {e}")
        exit(1)


if __name__ == "__main__":
    main()
