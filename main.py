#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
AIæ–°é—»æ—¥æŠ¥æœºå™¨äººä¸»ç¨‹åº
æ¯æ—¥è‡ªåŠ¨èšåˆAIç›¸å…³æ–°é—»å¹¶æ¨é€åˆ°é’‰é’‰
"""

import json
import logging
import requests
import feedparser
import hmac
import hashlib
import time
import base64
from datetime import datetime, timedelta
from typing import List, Dict, Optional
from concurrent.futures import ThreadPoolExecutor, as_completed

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('ai_news_bot.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


class NewsItem:
    """æ–°é—»æ¡ç›®æ•°æ®ç»“æ„"""
    
    def __init__(self, title: str, summary: str, url: str, source: str, 
                 published: datetime, category: str, importance_score: float = 0.0):
        self.title = title
        self.summary = summary
        self.url = url
        self.source = source
        self.published = published
        self.category = category
        self.importance_score = importance_score


class NewsAggregator:
    """æ–°é—»èšåˆå™¨"""
    
    def __init__(self, config_path: str = "config.json"):
        with open(config_path, 'r', encoding='utf-8') as f:
            self.config = json.load(f)
        
        self.webhook_url = self.config["dingtalk_webhook"]
        self.secret = self.config.get("dingtalk_secret", "")
        self.sent_urls_file = "sent_urls.json"
        self.sent_urls = self._load_sent_urls()
        
    def _load_sent_urls(self) -> set:
        """åŠ è½½å·²å‘é€çš„æ–°é—»URL"""
        try:
            with open(self.sent_urls_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                return set(data.get('sent_urls', []))
        except FileNotFoundError:
            return set()
    
    def _save_sent_urls(self):
        """ä¿å­˜å·²å‘é€çš„æ–°é—»URL"""
        data = {
            'sent_urls': list(self.sent_urls),
            'last_update': datetime.now().isoformat()
        }
        with open(self.sent_urls_file, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
    
    def _get_url_hash(self, url: str) -> str:
        """ç”ŸæˆURLçš„å“ˆå¸Œå€¼ç”¨äºå»é‡"""
        return hashlib.md5(url.encode()).hexdigest()
    
    def _is_duplicate(self, url: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦é‡å¤æ–°é—»"""
        url_hash = self._get_url_hash(url)
        return url_hash in self.sent_urls
    
    def _calculate_importance_score(self, title: str, keywords: List[str]) -> float:
        """è®¡ç®—æ–°é—»é‡è¦æ€§è¯„åˆ†"""
        score = 0.0
        title_lower = title.lower()
        
        # åŸºç¡€åˆ†æ•°
        score += 1.0
        
        # å…³é”®è¯åŒ¹é…åŠ åˆ†
        for keyword in keywords:
            if keyword.lower() in title_lower:
                score += 2.0
        
        # ç‰¹æ®Šå…³é”®è¯é¢å¤–åŠ åˆ†
        high_value_keywords = [
            'openai', 'gpt-4', 'claude', 'anthropic', 'funding', 'acquisition',
            'breakthrough', 'research', 'paper'
        ]
        for keyword in high_value_keywords:
            if keyword in title_lower:
                score += 3.0
        
        return score
    
    def _fetch_rss_feed(self, source: Dict) -> List[NewsItem]:
        """æŠ“å–RSSæº"""
        try:
            logger.info("æ­£åœ¨æŠ“å–: " + source['name'])
            
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            
            response = requests.get(source['url'], headers=headers, timeout=15)
            response.raise_for_status()
            
            feed = feedparser.parse(response.content)
            news_items = []
            
            cutoff_time = datetime.now() - timedelta(hours=24)
            
            for entry in feed.entries[:50]:
                try:
                    published = datetime.now()
                    if hasattr(entry, 'published_parsed') and entry.published_parsed:
                        published = datetime(*entry.published_parsed[:6])
                    elif hasattr(entry, 'updated_parsed') and entry.updated_parsed:
                        published = datetime(*entry.updated_parsed[:6])
                    
                    if published < cutoff_time:
                        continue
                    
                    title = getattr(entry, 'title', '').strip()
                    url = getattr(entry, 'link', '').strip()
                    
                    if not title or not url:
                        continue
                    
                    if self._is_duplicate(url):
                        continue
                    
                    importance_score = self._calculate_importance_score(
                        title, source.get('keywords', [])
                    )
                    
                    summary = getattr(entry, 'summary', '')
                    if not summary and hasattr(entry, 'description'):
                        summary = getattr(entry, 'description', '')
                    
                    summary = self._clean_html(summary)
                    
                    news_item = NewsItem(
                        title=title,
                        summary=summary[:200] + "..." if len(summary) > 200 else summary,
                        url=url,
                        source=source['name'],
                        published=published,
                        category=source.get('category', 'general'),
                        importance_score=importance_score
                    )
                    
                    news_items.append(news_item)
                    
                except Exception as e:
                    logger.warning("å¤„ç†æ–°é—»æ¡ç›®æ—¶å‡ºé”™: " + str(e))
                    continue
            
            logger.info("ä» " + source['name'] + " æŠ“å–åˆ° " + str(len(news_items)) + " æ¡æ–°é—»")
            return news_items
            
        except Exception as e:
            logger.error("æŠ“å– RSS æºå¤±è´¥ " + source['name'] + ": " + str(e))
            return []
    
    def _clean_html(self, text: str) -> str:
        """æ¸…ç†HTMLæ ‡ç­¾"""
        import re
        text = re.sub(r'<[^>]+>', '', text)
        text = re.sub(r'\s+', ' ', text).strip()
        return text
    
    def fetch_all_news(self) -> List[NewsItem]:
        """æŠ“å–æ‰€æœ‰æ–°é—»æº"""
        all_news = []
        
        with ThreadPoolExecutor(max_workers=5) as executor:
            future_to_source = {}
            
            for category, sources in self.config["news_sources"].items():
                for source in sources:
                    source['category'] = category
                    future = executor.submit(self._fetch_rss_feed, source)
                    future_to_source[future] = source
            
            for future in as_completed(future_to_source):
                source = future_to_source[future]
                try:
                    news_items = future.result()
                    all_news.extend(news_items)
                except Exception as e:
                    logger.error("è·å–æ–°é—»æ—¶å‡ºé”™ " + source['name'] + ": " + str(e))
        
        all_news.sort(key=lambda x: x.importance_score, reverse=True)
        
        logger.info("æ€»å…±æŠ“å–åˆ° " + str(len(all_news)) + " æ¡æ–°é—»")
        return all_news


class AISummarizer:
    """AIæ–°é—»æ‘˜è¦å™¨"""
    
    def __init__(self, config: Dict):
        self.api_key = config["ai_summary"]["api_key"]
        self.model = config["ai_summary"]["model"]
        self.max_tokens = config["ai_summary"]["max_tokens"]
        self.temperature = config["ai_summary"]["temperature"]
        
    def generate_summary(self, news_item: NewsItem) -> str:
        """ä½¿ç”¨AIç”Ÿæˆæ–°é—»æ‘˜è¦"""
        if not self.api_key or self.api_key == "YOUR_OPENAI_API_KEY":
            return news_item.summary or "æš‚æ— æ‘˜è¦"
        
        try:
            prompt = "è¯·ä¸ºä»¥ä¸‹AIç›¸å…³æ–°é—»ç”Ÿæˆä¸€ä¸ªç®€æ´çš„ä¸­æ–‡æ‘˜è¦ï¼Œè¦æ±‚ï¼š\n"
            prompt += "1. é•¿åº¦æ§åˆ¶åœ¨50å­—ä»¥å†…\n"
            prompt += "2. ä¿ç•™å…³é”®ä¿¡æ¯ï¼ˆå…¬å¸åã€æŠ€æœ¯åã€æ•°æ®ç­‰ï¼‰\n"
            prompt += "3. è¯­è¨€å®¢è§‚ä¸“ä¸š\n\n"
            prompt += "æ–°é—»æ ‡é¢˜ï¼š" + news_item.title + "\n"
            prompt += "æ–°é—»å†…å®¹ï¼š" + news_item.summary + "\n\n"
            prompt += "è¯·ç›´æ¥è¾“å‡ºæ‘˜è¦ï¼Œä¸éœ€è¦å…¶ä»–è¯´æ˜ã€‚"
            
            headers = {
                "Authorization": "Bearer " + self.api_key,
                "Content-Type": "application/json"
            }
            
            data = {
                "model": self.model,
                "messages": [
                    {"role": "user", "content": prompt}
                ],
                "max_tokens": self.max_tokens,
                "temperature": self.temperature
            }
            
            response = requests.post(
                "https://api.openai.com/v1/chat/completions",
                headers=headers,
                json=data,
                timeout=30
            )
            
            response.raise_for_status()
            result = response.json()
            
            if "choices" in result and result["choices"]:
                return result["choices"][0]["message"]["content"].strip()
            else:
                return news_item.summary or "æš‚æ— æ‘˜è¦"
                
        except Exception as e:
            logger.error("AIæ‘˜è¦ç”Ÿæˆå¤±è´¥: " + str(e))
            return news_item.summary or "æš‚æ— æ‘˜è¦"


class DingTalkNotifier:
    """é’‰é’‰é€šçŸ¥å™¨"""
    
    def __init__(self, webhook_url: str, secret: str = ""):
        self.webhook_url = webhook_url
        self.secret = secret
    
    def _get_sign(self) -> str:
        """ç”Ÿæˆç­¾åï¼ˆå¦‚æœé…ç½®äº†secretï¼‰"""
        if not self.secret:
            return ""
        
        timestamp = str(int(time.time() * 1000))
        string_to_sign = timestamp + "\n" + self.secret
        hmac_code = hmac.new(
            self.secret.encode('utf-8'),
            string_to_sign.encode('utf-8'),
            digestmod=hashlib.sha256
        ).digest()
        
        sign = base64.b64encode(hmac_code).decode('utf-8')
        return "&timestamp=" + timestamp + "&sign=" + sign
    
    def send_daily_news(self, news_items: List[NewsItem], date: str) -> bool:
        """å‘é€æ¯æ—¥æ–°é—»"""
        try:
            message = self._build_markdown_message(news_items, date)
            
            data = {
                "msgtype": "markdown",
                "markdown": {
                    "title": "AIæ¯æ—¥æ—©æŠ¥ | " + date,
                    "text": message
                }
            }
            
            headers = {
                "Content-Type": "application/json"
            }
            
            url = self.webhook_url
            if self.secret:
                url += self._get_sign()
            
            response = requests.post(url, headers=headers, json=data, timeout=30)
            response.raise_for_status()
            
            result = response.json()
            if result.get("errcode") == 0:
                logger.info("é’‰é’‰æ¶ˆæ¯å‘é€æˆåŠŸ")
                return True
            else:
                logger.error("é’‰é’‰æ¶ˆæ¯å‘é€å¤±è´¥: " + result.get('errmsg'))
                return False
            
        except Exception as e:
            logger.error("é’‰é’‰æ¶ˆæ¯å‘é€å¤±è´¥: " + str(e))
            return False
    
    def _build_markdown_message(self, news_items: List[NewsItem], date: str) -> str:
        """æ„å»ºMarkdownæ ¼å¼çš„æ¶ˆæ¯"""
        message_parts = []
        
        message_parts.append("## ğŸ“… AI æ¯æ—¥æ—©æŠ¥ | " + date)
        message_parts.append("")
        message_parts.append("**ä»Šæ—¥ç²¾é€‰ " + str(len(news_items)) + " æ¡AIè¦é—»**")
        message_parts.append("")
        
        for i, news in enumerate(news_items, 1):
            emoji = self._get_category_emoji(news.category)
            
            news_line = "**" + emoji + " " + news.title + "**\n"
            news_line += "> " + news.summary + "\n"
            news_line += "> ğŸ“° æ¥æºï¼š" + news.source + "\n"
            news_line += "> ğŸ”— [åŸæ–‡é“¾æ¥](" + news.url + ")\n"
            
            message_parts.append(news_line)
        
        message_parts.append("")
        message_parts.append("---")
        message_parts.append("*æœ¬ç®€æŠ¥ç”± AI è‡ªåŠ¨ç”Ÿæˆï¼Œä»…ä¾›å†…éƒ¨å‚è€ƒ*")
        
        return "\n".join(message_parts)
    
    def _get_category_emoji(self, category: str) -> str:
        """æ ¹æ®ç±»åˆ«è¿”å›å¯¹åº”çš„emoji"""
        emoji_map = {
            "international_media": "ğŸŒ",
            "chinese_media": "ğŸ‡¨ğŸ‡³", 
            "ai_funding": "ğŸ’°",
            "general": "ğŸ“°"
        }
        return emoji_map.get(category, "ğŸ“°")


class AINewsBot:
    """AIæ–°é—»æœºå™¨äººä¸»ç±»"""
    
    def __init__(self, config_path: str = "config.json"):
        self.config_path = config_path
        self.aggregator = NewsAggregator(config_path)
        self.summarizer = AISummarizer(self.aggregator.config)
        self.notifier = DingTalkNotifier(
            self.aggregator.webhook_url,
            self.aggregator.secret
        )
    
    def run_daily_job(self) -> bool:
        """æ‰§è¡Œæ¯æ—¥æ–°é—»æ”¶é›†å’Œæ¨é€ä»»åŠ¡"""
        try:
            logger.info("å¼€å§‹æ‰§è¡Œæ¯æ—¥æ–°é—»ä»»åŠ¡")
            
            all_news = self.aggregator.fetch_all_news()
            
            if not all_news:
                logger.warning("æœªæŠ“å–åˆ°ä»»ä½•æ–°é—»")
                return False
            
            max_news = self.aggregator.config["settings"]["max_news"]
            selected_news = all_news[:max_news]
            
            logger.info("ç­›é€‰å‡º " + str(len(selected_news)) + " æ¡é«˜è´¨é‡æ–°é—»")
            
            for news in selected_news:
                news.summary = self.summarizer.generate_summary(news)
                time.sleep(1)
            
            current_date = datetime.now().strftime("%Yå¹´%mæœˆ%dæ—¥")
            success = self.notifier.send_daily_news(selected_news, current_date)
            
            if success:
                for news in selected_news:
                    self.aggregator.sent_urls.add(
                        self.aggregator._get_url_hash(news.url)
                    )
                self.aggregator._save_sent_urls()
                
                logger.info("æ¯æ—¥æ–°é—»ä»»åŠ¡æ‰§è¡Œå®Œæˆ")
                return True
            else:
                logger.error("å‘é€æ¶ˆæ¯å¤±è´¥")
                return False
                
        except Exception as e:
            logger.error("æ‰§è¡Œæ¯æ—¥ä»»åŠ¡æ—¶å‡ºé”™: " + str(e))
            return False


def main():
    """ä¸»å‡½æ•°"""
    try:
        bot = AINewsBot()
        success = bot.run_daily_job()
        
        if success:
            logger.info("ç¨‹åºæ‰§è¡ŒæˆåŠŸ")
            exit(0)
        else:
            logger.error("ç¨‹åºæ‰§è¡Œå¤±è´¥")
            exit(1)
            
    except Exception as e:
        logger.error("ç¨‹åºå¼‚å¸¸: " + str(e))
        exit(1)


if __name__ == "__main__":
    main()
