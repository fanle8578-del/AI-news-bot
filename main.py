#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
AIæ–°é—»æ—¥æŠ¥æœºå™¨äºº - é’‰é’‰ç‰ˆ
æ¯æ—¥è‡ªåŠ¨èšåˆAIç›¸å…³æ–°é—»å¹¶æ¨é€åˆ°é’‰é’‰
"""

import json
import logging
import requests
import feedparser
import time
from datetime import datetime, timedelta
from typing import List, Dict
from concurrent.futures import ThreadPoolExecutor, as_completed

# æ—¥å¿—é…ç½®
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class NewsItem:
    """æ–°é—»æ¡ç›®"""
    def __init__(self, title, summary, url, source, published, category, importance_score=0.0):
        self.title = title
        self.summary = summary
        self.url = url
        self.source = source
        self.published = published
        self.category = category
        self.importance_score = importance_score


class NewsAggregator:
    """æ–°é—»èšåˆå™¨"""
    def __init__(self, config_path="config.json"):
        with open(config_path, 'r', encoding='utf-8') as f:
            self.config = json.load(f)
        
        self.webhook_url = self.config["dingtalk_webhook"]
        self.secret = self.config.get("dingtalk_secret", "")
        self.sent_urls = self._load_sent_urls()
        
    def _load_sent_urls(self):
        try:
            with open("sent_urls.json", 'r', encoding='utf-8') as f:
                return set(json.load(f).get('sent_urls', []))
        except:
            return set()
    
    def _save_sent_urls(self):
        with open("sent_urls.json", 'w', encoding='utf-8') as f:
            json.dump({"sent_urls": list(self.sent_urls)}, f)
    
    def _get_url_hash(self, url):
        import hashlib
        return hashlib.md5(url.encode()).hexdigest()
    
    def _is_duplicate(self, url):
        return self._get_url_hash(url) in self.sent_urls
    
    def _calculate_score(self, title, keywords):
        score = 1.0
        title_lower = title.lower()
        for kw in keywords:
            if kw.lower() in title_lower:
                score += 2.0
        for kw in ['openai', 'gpt', 'funding', 'research', 'breakthrough']:
            if kw in title_lower:
                score += 3.0
        return score
    
    def _fetch_rss(self, source):
        try:
            logger.info("æ­£åœ¨æŠ“å–: " + source['name'])
            headers = {'User-Agent': 'Mozilla/5.0'}
            response = requests.get(source['url'], headers=headers, timeout=15)
            response.raise_for_status()
            
            feed = feedparser.parse(response.content)
            news_list = []
            cutoff = datetime.now() - timedelta(hours=24)
            
            for entry in feed.entries[:30]:
                try:
                    published = datetime.now()
                    if hasattr(entry, 'published_parsed') and entry.published_parsed:
                        published = datetime(*entry.published_parsed[:6])
                    
                    if published < cutoff:
                        continue
                    
                    title = getattr(entry, 'title', '').strip()
                    url = getattr(entry, 'link', '').strip()
                    
                    if not title or not url or self._is_duplicate(url):
                        continue
                    
                    import re
                    summary = getattr(entry, 'summary', '')
                    summary = re.sub(r'<[^>]+>', '', summary)
                    summary = re.sub(r'\s+', ' ', summary).strip()[:200]
                    
                    news = NewsItem(
                        title=title,
                        summary=summary + "...",
                        url=url,
                        source=source['name'],
                        published=published,
                        category=source.get('category', 'general'),
                        importance_score=self._calculate_score(title, source.get('keywords', []))
                    )
                    news_list.append(news)
                except Exception as e:
                    continue
            
            logger.info("ä» " + source['name'] + " æŠ“å–åˆ° " + str(len(news_list)) + " æ¡æ–°é—»")
            return news_list
        except Exception as e:
            logger.error("æŠ“å–å¤±è´¥ " + source['name'] + ": " + str(e))
            return []
    
    def fetch_all(self):
        all_news = []
        with ThreadPoolExecutor(max_workers=3) as executor:
            futures = []
            for category, sources in self.config["news_sources"].items():
                for src in sources:
                    src['category'] = category
                    futures.append(executor.submit(self._fetch_rss, src))
            
            for f in futures:
                try:
                    all_news.extend(f.result())
                except:
                    pass
        
        all_news.sort(key=lambda x: x.importance_score, reverse=True)
        logger.info("æ€»å…±æŠ“å–åˆ° " + str(len(all_news)) + " æ¡æ–°é—»")
        return all_news


class DingTalkSender:
    """é’‰é’‰å‘é€å™¨"""
    def __init__(self, webhook_url, secret=""):
        self.webhook_url = webhook_url
        self.secret = secret
    
    def _sign(self):
        """ç”Ÿæˆç­¾å"""
        if not self.secret:
            return ""
        
        timestamp = str(int(time.time() * 1000))
        string = timestamp + "\n" + self.secret
        
        import hmac
        import base64
        import hashlib
        
        signature = hmac.new(
            self.secret.encode('utf-8'),
            string.encode('utf-8'),
            digestmod=hashlib.sha256
        ).digest()
        
        sign = base64.b64encode(signature).decode('utf-8')
        return "&timestamp=" + timestamp + "&sign=" + sign
    
    def send(self, news_list, date_str):
        try:
            # æ„å»ºMarkdownæ¶ˆæ¯
            text = self._build_message(news_list, date_str)
            
            payload = {
                "msgtype": "markdown",
                "markdown": {
                    "title": "AIæ¯æ—¥æ—©æŠ¥ | " + date_str,
                    "text": text
                }
            }
            
            url = self.webhook_url + self._sign()
            
            headers = {"Content-Type": "application/json"}
            response = requests.post(url, json=payload, headers=headers, timeout=30)
            
            result = response.json()
            
            if result.get("errcode") == 0:
                logger.info("é’‰é’‰æ¶ˆæ¯å‘é€æˆåŠŸ")
                return True
            else:
                logger.error("é’‰é’‰å‘é€å¤±è´¥: " + str(result))
                # å¦‚æœåŠ ç­¾å¤±è´¥ï¼Œå°è¯•ä¸ä½¿ç”¨ç­¾å
                if "ç­¾åä¸åŒ¹é…" in str(result.get("errmsg", "")):
                    logger.info("å°è¯•ä¸ä½¿ç”¨ç­¾åé‡æ–°å‘é€...")
                    response = requests.post(self.webhook_url, json=payload, headers=headers, timeout=30)
                    result = response.json()
                    if result.get("errcode") == 0:
                        logger.info("ä¸ä½¿ç”¨ç­¾åå‘é€æˆåŠŸ")
                        return True
                return False
                
        except Exception as e:
            logger.error("å‘é€å¼‚å¸¸: " + str(e))
            return False
    
    def _build_message(self, news_list, date_str):
        lines = []
        lines.append("## ğŸ“… AI æ¯æ—¥æ—©æŠ¥ | " + date_str)
        lines.append("")
        lines.append("**ä»Šæ—¥ç²¾é€‰ " + str(len(news_list)) + " æ¡AIè¦é—»**")
        lines.append("")
        
        for i, news in enumerate(news_list, 1):
            emoji_map = {"international_media": "ğŸŒ", "chinese_media": "ğŸ‡¨ğŸ‡³", "ai_funding": "ğŸ’°"}
            emoji = emoji_map.get(news.category, "ğŸ“°")
            
            lines.append("**" + emoji + " " + news.title + "**")
            lines.append("> " + news.summary)
            lines.append("> ğŸ“° " + news.source + " | [ğŸ”—åŸæ–‡](" + news.url + ")")
            lines.append("")
        
        lines.append("---")
        lines.append("*æœ¬ç®€æŠ¥ç”± AI è‡ªåŠ¨ç”Ÿæˆ*")
        
        return "\n".join(lines)


class AINewsBot:
    """ä¸»æœºå™¨äºº"""
    def __init__(self):
        self.aggregator = NewsAggregator()
        self.sender = DingTalkSender(self.aggregator.webhook_url, self.aggregator.secret)
    
    def run(self):
        try:
            logger.info("å¼€å§‹æ‰§è¡Œæ¯æ—¥æ–°é—»ä»»åŠ¡")
            
            all_news = self.aggregator.fetch_all()
            if not all_news:
                logger.warning("æœªæŠ“å–åˆ°ä»»ä½•æ–°é—»")
                return False
            
            max_news = self.aggregator.config["settings"]["max_news"]
            selected = all_news[:max_news]
            logger.info("ç­›é€‰å‡º " + str(len(selected)) + " æ¡é«˜è´¨é‡æ–°é—»")
            
            date_str = datetime.now().strftime("%Yå¹´%mæœˆ%dæ—¥")
            success = self.sender.send(selected, date_str)
            
            if success:
                for news in selected:
                    self.aggregator.sent_urls.add(self.aggregator._get_url_hash(news.url))
                self.aggregator._save_sent_urls()
                logger.info("ä»»åŠ¡æ‰§è¡Œå®Œæˆ")
                return True
            else:
                logger.error("å‘é€æ¶ˆæ¯å¤±è´¥")
                return False
                
        except Exception as e:
            logger.error("æ‰§è¡Œå¼‚å¸¸: " + str(e))
            return False


def main():
    bot = AINewsBot()
    success = bot.run()
    exit(0 if success else 1)


if __name__ == "__main__":
    main()
