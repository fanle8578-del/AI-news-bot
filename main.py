#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
AIæ–°é—»æ—¥æŠ¥æœºå™¨äºº - ä¼˜åŒ–ç‰ˆ
èšç„¦ï¼šä¸–ç•Œæ¨¡å‹ã€AIç®—åŠ›ã€å¤´éƒ¨å…¬å¸åŠ¨æ€ã€èèµ„èµ„è®¯
"""

import json
import logging
import requests
import feedparser
import time
import re
from datetime import datetime, timedelta
from typing import List, Dict
from concurrent.futures import ThreadPoolExecutor, as_completed

# æ—¥å¿—é…ç½®
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class NewsItem:
    """æ–°é—»æ¡ç›®"""
    def __init__(self, title, summary, url, source, published, category, importance_score=0.0):
        self.title = title
        self.summary = summary
        self.url = url
        self.source = source
        self.published = published
        self.category = category
        self.importance_score = importance_score


class NewsAggregator:
    """æ–°é—»èšåˆå™¨ - èšç„¦ç‰ˆ"""
    def __init__(self, config_path="config.json"):
        with open(config_path, 'r', encoding='utf-8') as f:
            self.config = json.load(f)
        
        self.webhook_url = self.config["dingtalk_webhook"]
        self.secret = self.config.get("dingtalk_secret", "")
        self.sent_urls = self._load_sent_urls()
        
    def _load_sent_urls(self):
        try:
            with open("sent_urls.json", 'r', encoding='utf-8') as f:
                return set(json.load(f).get('sent_urls', []))
        except:
            return set()
    
    def _save_sent_urls(self):
        with open("sent_urls.json", 'w', encoding='utf-8') as f:
            json.dump({"sent_urls": list(self.sent_urls)}, f)
    
    def _get_url_hash(self, url):
        import hashlib
        return hashlib.md5(url.encode()).hexdigest()
    
    def _is_duplicate(self, url):
        return self._get_url_hash(url) in self.sent_urls
    
    def _calculate_score(self, title):
        """è®¡ç®—æ–°é—»é‡è¦æ€§è¯„åˆ† - èšç„¦é¢†åŸŸåŠ æƒ"""
        score = 1.0
        title_lower = title.lower()
        
        # é«˜æƒé‡å…³é”®è¯ï¼ˆå¤´éƒ¨AIå…¬å¸ï¼‰
        high_weight = ['openai', 'anthropic', 'google deepmind', 'meta ai', 'claude', 'gpt-5', 'gpt-4', 'sora', 'gemini']
        for kw in high_weight:
            if kw in title_lower:
                score += 5.0
        
        # ä¸­æƒé‡å…³é”®è¯ï¼ˆæ ¸å¿ƒé¢†åŸŸï¼‰
        mid_weight = ['world model', 'world model', 'AI compute', 'AI chips', 'GPU', 'Nvidia', 'funding', 'Series A', 'Series B', 'Series C', 'data annotation', 'human labeling', 'AI startup']
        for kw in mid_weight:
            if kw in title_lower:
                score += 3.0
        
        # ä¸€èˆ¬æƒé‡å…³é”®è¯
        general = ['generative AI', 'LLM', 'multimodal', 'AI infrastructure', 'AI investment']
        for kw in general:
            if kw in title_lower:
                score += 1.5
        
        return score
    
    def _fetch_rss(self, source):
        try:
            logger.info("æ­£åœ¨æŠ“å–: " + source['name'])
            headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}
            response = requests.get(source['url'], headers=headers, timeout=15)
            response.raise_for_status()
            
            feed = feedparser.parse(response.content)
            news_list = []
            cutoff = datetime.now() - timedelta(hours=24)
            
            for entry in feed.entries[:50]:
                try:
                    published = datetime.now()
                    if hasattr(entry, 'published_parsed') and entry.published_parsed:
                        published = datetime(*entry.published_parsed[:6])
                    
                    if published < cutoff:
                        continue
                    
                    title = getattr(entry, 'title', '').strip()
                    url = getattr(entry, 'link', '').strip()
                    
                    if not title or not url or self._is_duplicate(url):
                        continue
                    
                    # è®¡ç®—é‡è¦æ€§
                    importance = self._calculate_score(title)
                    
                    # ç”Ÿæˆæ‘˜è¦
                    summary = getattr(entry, 'summary', '')
                    summary = re.sub(r'<[^>]+>', '', summary)
                    summary = re.sub(r'\s+', ' ', summary).strip()[:250]
                    
                    news = NewsItem(
                        title=title,
                        summary=summary + "...",
                        url=url,
                        source=source['name'],
                        published=published,
                        category=source.get('category', 'general'),
                        importance_score=importance
                    )
                    news_list.append(news)
                except Exception as e:
                    continue
            
            logger.info("ä» " + source['name'] + " æŠ“å–åˆ° " + str(len(news_list)) + " æ¡ç›¸å…³æ–°é—»")
            return news_list
        except Exception as e:
            logger.error("æŠ“å–å¤±è´¥ " + source['name'] + ": " + str(e))
            return []
    
    def fetch_all(self):
        all_news = []
        with ThreadPoolExecutor(max_workers=4) as executor:
            futures = []
            for category, sources in self.config["news_sources"].items():
                for src in sources:
                    src['category'] = category
                    futures.append(executor.submit(self._fetch_rss, src))
            
            for f in futures:
                try:
                    all_news.extend(f.result())
                except:
                    pass
        
        # æŒ‰é‡è¦æ€§æ’åº
        all_news.sort(key=lambda x: x.importance_score, reverse=True)
        logger.info("æ€»å…±ç­›é€‰å‡º " + str(len(all_news)) + " æ¡é«˜è´¨é‡æ–°é—»")
        return all_news


class DingTalkSender:
    """é’‰é’‰å‘é€å™¨ - UIä¼˜åŒ–ç‰ˆ"""
    def __init__(self, webhook_url, secret=""):
        self.webhook_url = webhook_url
        self.secret = secret
    
    def _sign(self):
        if not self.secret:
            return ""
        
        timestamp = str(int(time.time() * 1000))
        string = timestamp + "\n" + self.secret
        
        import hmac
        import base64
        import hashlib
        
        signature = hmac.new(
            self.secret.encode('utf-8'),
            string.encode('utf-8'),
            digestmod=hashlib.sha256
        ).digest()
        
        sign = base64.b64encode(signature).decode('utf-8')
        return "&timestamp=" + timestamp + "&sign=" + sign
    
    def send(self, news_list, date_str):
        try:
            text = self._build_message(news_list, date_str)
            
            payload = {
                "msgtype": "markdown",
                "markdown": {
                    "title": "ğŸ¤– AI Daily Brief | " + date_str,
                    "text": text
                }
            }
            
            url = self.webhook_url + self._sign()
            headers = {"Content-Type": "application/json"}
            response = requests.post(url, json=payload, headers=headers, timeout=30)
            
            result = response.json()
            
            if result.get("errcode") == 0:
                logger.info("âœ… é’‰é’‰æ¶ˆæ¯å‘é€æˆåŠŸ")
                return True
            else:
                logger.error("âŒ é’‰é’‰å‘é€å¤±è´¥: " + str(result))
                return False
                
        except Exception as e:
            logger.error("âŒ å‘é€å¼‚å¸¸: " + str(e))
            return False
    
    def _build_message(self, news_list, date_str):
        """æ„å»ºä¼˜åŒ–åçš„UIç•Œé¢"""
        lines = []
        
        # ğŸ¯ æ ‡é¢˜åŒºåŸŸ
        lines.append("## ğŸ¤– AI Daily Brief")
        lines.append("### ğŸ“… " + date_str)
        lines.append("")
        
        # ğŸ“Š ç»Ÿè®¡ä¿¡æ¯
        lines.append("---")
        lines.append("ğŸ“ˆ **ä»Šæ—¥ç²¾é€‰ " + str(len(news_list)) + " æ¡æ ¸å¿ƒèµ„è®¯**")
        lines.append("")
        
        # æŒ‰ç±»åˆ«åˆ†ç»„ç»Ÿè®¡
        categories = {}
        for news in news_list:
            cat = news.category
            if cat not in categories:
                categories[cat] = []
            categories[cat].append(news)
        
        # ç±»åˆ«emojiæ˜ å°„
        emoji_map = {
            "ai_research": "ğŸ”¬",
            "ai_funding": "ğŸ’°", 
            "ai_compute": "âš¡",
            "ai_data": "ğŸ“Š",
            "ai_product": "ğŸš€",
            "general": "ğŸ“°"
        }
        
        category_names = {
            "ai_research": "å¤´éƒ¨å…¬å¸ç ”å‘",
            "ai_funding": "èèµ„åŠ¨æ€",
            "ai_compute": "ç®—åŠ›å¸‚åœº",
            "ai_data": "æ•°æ®æ ‡æ³¨",
            "ai_product": "AIåº”ç”¨",
            "general": "ç»¼åˆèµ„è®¯"
        }
        
        # æ˜¾ç¤ºå„ç±»åˆ«ç»Ÿè®¡
        stats_parts = []
        for cat, cat_news in categories.items():
            emoji = emoji_map.get(cat, "ğŸ“°")
            name = category_names.get(cat, "ç»¼åˆèµ„è®¯")
            stats_parts.append(emoji + " " + name + ": " + str(len(cat_news)))
        
        lines.append(" | ".join(stats_parts))
        lines.append("")
        lines.append("---")
        lines.append("")
        
        # ğŸ“° æ–°é—»è¯¦æƒ…
        lines.append("### ğŸ“° **ä»Šæ—¥è¦é—»**")
        lines.append("")
        
        for i, news in enumerate(news_list, 1):
            emoji = emoji_map.get(news.category, "ğŸ“°")
            
            # æ ‡é¢˜ï¼ˆåŠ ç²—ï¼‰
            lines.append("#### " + emoji + " **" + news.title + "**")
            
            # æ‘˜è¦ï¼ˆå¼•ç”¨æ ¼å¼ï¼‰
            lines.append("> " + news.summary)
            
            # æ¥æºå’Œé“¾æ¥
            lines.append("> ğŸ“ **" + news.source + "** | ğŸ”— [é˜…è¯»åŸæ–‡](" + news.url + ")")
            lines.append("")
        
        # ğŸ¯ åº•éƒ¨ä¿¡æ¯
        lines.append("---")
        lines.append("")
        lines.append("ğŸ’¡ **èšç„¦é¢†åŸŸ**: ä¸–ç•Œæ¨¡å‹ | AIç®—åŠ› | æ•°æ®æ ‡æ³¨ | å¤´éƒ¨å…¬å¸åŠ¨æ€ | èèµ„èµ„è®¯")
        lines.append("")
        lines.append("ğŸ¤– *æœ¬ç®€æŠ¥ç”± AI è‡ªåŠ¨ç”Ÿæˆï¼Œæ¯æ—¥09:30å®šæ—¶æ¨é€*")
        
        return "\n".join(lines)


class AINewsBot:
    """ä¸»æœºå™¨äºº - ä¼˜åŒ–ç‰ˆ"""
    def __init__(self):
        self.aggregator = NewsAggregator()
        self.sender = DingTalkSender(self.aggregator.webhook_url, self.aggregator.secret)
    
    def run(self):
        try:
            logger.info("ğŸš€ å¼€å§‹æ‰§è¡ŒAIæ–°é—»ä»»åŠ¡")
            logger.info("=" * 50)
            
            # æŠ“å–æ–°é—»
            all_news = self.aggregator.fetch_all()
            
            if not all_news:
                logger.warning("âš ï¸ æœªæŠ“å–åˆ°ä»»ä½•æ–°é—»")
                return False
            
            # ç­›é€‰top 10
            max_news = self.aggregator.config["settings"]["max_news"]
            selected = all_news[:max_news]
            
            logger.info("âœ… ç­›é€‰å‡º " + str(len(selected)) + " æ¡æ ¸å¿ƒèµ„è®¯")
            logger.info("=" * 50)
            
            # å‘é€
            date_str = datetime.now().strftime("%Y.%m.%d")
            success = self.sender.send(selected, date_str)
            
            if success:
                for news in selected:
                    self.aggregator.sent_urls.add(self.aggregator._get_url_hash(news.url))
                self.aggregator._save_sent_urls()
                logger.info("ğŸ‰ ä»»åŠ¡æ‰§è¡Œå®Œæˆï¼")
                return True
            else:
                logger.error("âŒ å‘é€æ¶ˆæ¯å¤±è´¥")
                return False
                
        except Exception as e:
            logger.error("âŒ æ‰§è¡Œå¼‚å¸¸: " + str(e))
            return False


def main():
    bot = AINewsBot()
    success = bot.run()
    exit(0 if success else 1)


if __name__ == "__main__":
    main()
